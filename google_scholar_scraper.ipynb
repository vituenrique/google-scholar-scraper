{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, random, json, math\n",
    "\n",
    "PAGE_SIZE = 10\n",
    "USER_AGENT_LIST = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_agent():\n",
    "    for _ in USER_AGENT_LIST:\n",
    "        #Pick a random user agent\n",
    "        user_agent = random.choice(USER_AGENT_LIST)\n",
    "        \n",
    "    return user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_number_pages(query:str, since:int=None, is_review:bool=False) -> int:\n",
    "    #Set the headers \n",
    "    headers = { 'User-Agent': get_user_agent() }\n",
    "    \n",
    "    params = {\n",
    "        'q': query,  # search query\n",
    "        'hl': 'en',       # language of the search\n",
    "        'as_ylo': since, \n",
    "        'as_rr': int(is_review)\n",
    "    }\n",
    "    html = requests.get('https://scholar.google.com/scholar', headers=headers, params=params).text\n",
    "    n_results = int(re.search(r'About (.*?) results', html).group(1))\n",
    "\n",
    "    return math.ceil(n_results/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_google_scholar(query:str, since:int=None, is_review:bool=False) -> list:\n",
    "\n",
    "    n_pages = scrape_number_pages(query=query, since=since, is_review=is_review)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for page in range(n_pages): \n",
    "        headers = { 'User-agent': get_user_agent() }\n",
    "        params = {\n",
    "            'start': page * PAGE_SIZE,\n",
    "            'q': query,  # search query\n",
    "            'hl': 'en',       # language of the search\n",
    "            'as_ylo': since, \n",
    "            'as_rr': int(is_review)\n",
    "        }\n",
    "        html = requests.get('https://scholar.google.com/scholar', headers=headers, params=params).text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Container where all needed data is located\n",
    "        for result in soup.select('.gs_r.gs_or.gs_scl'):\n",
    "            title = result.select_one('.gs_rt').text\n",
    "            try:\n",
    "                title_link = result.select_one('.gs_rt a')['href']\n",
    "            except:\n",
    "                title_link = None\n",
    "            publication_info = result.select_one('.gs_a').text\n",
    "            snippet = result.select_one('.gs_rs').text\n",
    "            cited_by = result.select_one('#gs_res_ccl_mid .gs_nph+ a')['href']\n",
    "            try:\n",
    "                pdf_link = result.select_one('.gs_or_ggsm a:nth-child(1)')['href']\n",
    "            except:\n",
    "                pdf_link = None\n",
    "                \n",
    "            data.append({\n",
    "                'title': title,\n",
    "                'title_link': title_link,\n",
    "                'publication_info': publication_info,\n",
    "                'snippet': snippet,\n",
    "                'cited_by': f'https://scholar.google.com{cited_by}',\n",
    "                \"pdf_link\": pdf_link\n",
    "            })\n",
    "    return data\n",
    "\n",
    "papers = scraper_google_scholar(\n",
    "    query='~chest ~caption \"report generation\" \"x ray\" -intitle:segmentation -intitle:classification -\"case study\" -\"case report\"',\n",
    "    since=2022, is_review=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148672"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"./papers.json\", \"w+\").write(json.dumps(papers, indent=4, ensure_ascii='utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
